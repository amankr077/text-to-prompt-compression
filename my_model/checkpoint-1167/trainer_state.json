{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 3.0,
  "eval_steps": 500,
  "global_step": 1167,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.02570694087403599,
      "grad_norm": 29.645320892333984,
      "learning_rate": 4.961439588688946e-05,
      "loss": 4.4561,
      "step": 10
    },
    {
      "epoch": 0.05141388174807198,
      "grad_norm": 26.17914581298828,
      "learning_rate": 4.91859468723222e-05,
      "loss": 3.6136,
      "step": 20
    },
    {
      "epoch": 0.07712082262210797,
      "grad_norm": 22.926612854003906,
      "learning_rate": 4.875749785775493e-05,
      "loss": 3.4842,
      "step": 30
    },
    {
      "epoch": 0.10282776349614396,
      "grad_norm": 28.824190139770508,
      "learning_rate": 4.8329048843187664e-05,
      "loss": 3.0449,
      "step": 40
    },
    {
      "epoch": 0.12853470437017994,
      "grad_norm": 25.575939178466797,
      "learning_rate": 4.790059982862039e-05,
      "loss": 3.1035,
      "step": 50
    },
    {
      "epoch": 0.15424164524421594,
      "grad_norm": 27.45604133605957,
      "learning_rate": 4.747215081405313e-05,
      "loss": 3.0145,
      "step": 60
    },
    {
      "epoch": 0.17994858611825193,
      "grad_norm": 24.459142684936523,
      "learning_rate": 4.7043701799485865e-05,
      "loss": 2.7218,
      "step": 70
    },
    {
      "epoch": 0.20565552699228792,
      "grad_norm": 25.411184310913086,
      "learning_rate": 4.6615252784918594e-05,
      "loss": 2.8121,
      "step": 80
    },
    {
      "epoch": 0.23136246786632392,
      "grad_norm": 24.00638198852539,
      "learning_rate": 4.618680377035133e-05,
      "loss": 2.4704,
      "step": 90
    },
    {
      "epoch": 0.2570694087403599,
      "grad_norm": 23.98120880126953,
      "learning_rate": 4.5758354755784066e-05,
      "loss": 3.0836,
      "step": 100
    },
    {
      "epoch": 0.2827763496143959,
      "grad_norm": 20.727556228637695,
      "learning_rate": 4.53299057412168e-05,
      "loss": 2.8921,
      "step": 110
    },
    {
      "epoch": 0.30848329048843187,
      "grad_norm": 21.529376983642578,
      "learning_rate": 4.490145672664953e-05,
      "loss": 2.6102,
      "step": 120
    },
    {
      "epoch": 0.3341902313624679,
      "grad_norm": 24.641874313354492,
      "learning_rate": 4.447300771208227e-05,
      "loss": 2.7754,
      "step": 130
    },
    {
      "epoch": 0.35989717223650386,
      "grad_norm": 26.84534454345703,
      "learning_rate": 4.4044558697514997e-05,
      "loss": 2.5824,
      "step": 140
    },
    {
      "epoch": 0.3856041131105398,
      "grad_norm": 20.37823486328125,
      "learning_rate": 4.361610968294773e-05,
      "loss": 2.5174,
      "step": 150
    },
    {
      "epoch": 0.41131105398457585,
      "grad_norm": 21.641681671142578,
      "learning_rate": 4.318766066838046e-05,
      "loss": 2.7118,
      "step": 160
    },
    {
      "epoch": 0.4370179948586118,
      "grad_norm": 21.064828872680664,
      "learning_rate": 4.27592116538132e-05,
      "loss": 2.6735,
      "step": 170
    },
    {
      "epoch": 0.46272493573264784,
      "grad_norm": 19.088651657104492,
      "learning_rate": 4.233076263924593e-05,
      "loss": 2.7149,
      "step": 180
    },
    {
      "epoch": 0.4884318766066838,
      "grad_norm": 19.756723403930664,
      "learning_rate": 4.190231362467866e-05,
      "loss": 2.4814,
      "step": 190
    },
    {
      "epoch": 0.5141388174807198,
      "grad_norm": 18.674158096313477,
      "learning_rate": 4.14738646101114e-05,
      "loss": 2.4015,
      "step": 200
    },
    {
      "epoch": 0.5398457583547558,
      "grad_norm": 21.37737464904785,
      "learning_rate": 4.1045415595544135e-05,
      "loss": 2.5028,
      "step": 210
    },
    {
      "epoch": 0.5655526992287918,
      "grad_norm": 23.799928665161133,
      "learning_rate": 4.0616966580976864e-05,
      "loss": 2.4682,
      "step": 220
    },
    {
      "epoch": 0.5912596401028277,
      "grad_norm": 17.907590866088867,
      "learning_rate": 4.01885175664096e-05,
      "loss": 2.4253,
      "step": 230
    },
    {
      "epoch": 0.6169665809768637,
      "grad_norm": 17.936176300048828,
      "learning_rate": 3.9760068551842336e-05,
      "loss": 2.6185,
      "step": 240
    },
    {
      "epoch": 0.6426735218508998,
      "grad_norm": 18.837514877319336,
      "learning_rate": 3.9331619537275065e-05,
      "loss": 2.7548,
      "step": 250
    },
    {
      "epoch": 0.6683804627249358,
      "grad_norm": 17.89991569519043,
      "learning_rate": 3.89031705227078e-05,
      "loss": 2.6015,
      "step": 260
    },
    {
      "epoch": 0.6940874035989717,
      "grad_norm": 17.20867919921875,
      "learning_rate": 3.847472150814053e-05,
      "loss": 2.4953,
      "step": 270
    },
    {
      "epoch": 0.7197943444730077,
      "grad_norm": 18.72536277770996,
      "learning_rate": 3.8046272493573266e-05,
      "loss": 2.4228,
      "step": 280
    },
    {
      "epoch": 0.7455012853470437,
      "grad_norm": 14.030342102050781,
      "learning_rate": 3.7617823479005996e-05,
      "loss": 2.1678,
      "step": 290
    },
    {
      "epoch": 0.7712082262210797,
      "grad_norm": 18.885643005371094,
      "learning_rate": 3.718937446443873e-05,
      "loss": 2.6446,
      "step": 300
    },
    {
      "epoch": 0.7969151670951157,
      "grad_norm": 15.317384719848633,
      "learning_rate": 3.676092544987147e-05,
      "loss": 2.4197,
      "step": 310
    },
    {
      "epoch": 0.8226221079691517,
      "grad_norm": 16.26556968688965,
      "learning_rate": 3.6332476435304203e-05,
      "loss": 2.502,
      "step": 320
    },
    {
      "epoch": 0.8483290488431876,
      "grad_norm": 15.140028953552246,
      "learning_rate": 3.590402742073693e-05,
      "loss": 2.3837,
      "step": 330
    },
    {
      "epoch": 0.8740359897172236,
      "grad_norm": 18.81905746459961,
      "learning_rate": 3.547557840616967e-05,
      "loss": 2.5153,
      "step": 340
    },
    {
      "epoch": 0.8997429305912596,
      "grad_norm": 15.822944641113281,
      "learning_rate": 3.5047129391602405e-05,
      "loss": 2.4089,
      "step": 350
    },
    {
      "epoch": 0.9254498714652957,
      "grad_norm": 14.639360427856445,
      "learning_rate": 3.4618680377035134e-05,
      "loss": 2.2289,
      "step": 360
    },
    {
      "epoch": 0.9511568123393316,
      "grad_norm": 13.512356758117676,
      "learning_rate": 3.419023136246787e-05,
      "loss": 2.1972,
      "step": 370
    },
    {
      "epoch": 0.9768637532133676,
      "grad_norm": 14.773947715759277,
      "learning_rate": 3.37617823479006e-05,
      "loss": 2.2716,
      "step": 380
    },
    {
      "epoch": 1.0025706940874035,
      "grad_norm": 14.346137046813965,
      "learning_rate": 3.3333333333333335e-05,
      "loss": 2.8202,
      "step": 390
    },
    {
      "epoch": 1.0282776349614395,
      "grad_norm": 13.004189491271973,
      "learning_rate": 3.2904884318766064e-05,
      "loss": 1.9479,
      "step": 400
    },
    {
      "epoch": 1.0539845758354756,
      "grad_norm": 15.418088912963867,
      "learning_rate": 3.247643530419881e-05,
      "loss": 2.2004,
      "step": 410
    },
    {
      "epoch": 1.0796915167095116,
      "grad_norm": 14.314446449279785,
      "learning_rate": 3.2047986289631536e-05,
      "loss": 1.7617,
      "step": 420
    },
    {
      "epoch": 1.1053984575835476,
      "grad_norm": 14.756770133972168,
      "learning_rate": 3.161953727506427e-05,
      "loss": 1.9898,
      "step": 430
    },
    {
      "epoch": 1.1311053984575836,
      "grad_norm": 16.84353256225586,
      "learning_rate": 3.1191088260497e-05,
      "loss": 1.8271,
      "step": 440
    },
    {
      "epoch": 1.1568123393316196,
      "grad_norm": 17.77546501159668,
      "learning_rate": 3.076263924592974e-05,
      "loss": 1.966,
      "step": 450
    },
    {
      "epoch": 1.1825192802056554,
      "grad_norm": 17.683202743530273,
      "learning_rate": 3.033419023136247e-05,
      "loss": 1.9055,
      "step": 460
    },
    {
      "epoch": 1.2082262210796915,
      "grad_norm": 16.582862854003906,
      "learning_rate": 2.9905741216795202e-05,
      "loss": 1.7515,
      "step": 470
    },
    {
      "epoch": 1.2339331619537275,
      "grad_norm": 15.756184577941895,
      "learning_rate": 2.9477292202227935e-05,
      "loss": 1.9217,
      "step": 480
    },
    {
      "epoch": 1.2596401028277635,
      "grad_norm": 15.827706336975098,
      "learning_rate": 2.9048843187660668e-05,
      "loss": 1.7071,
      "step": 490
    },
    {
      "epoch": 1.2853470437017995,
      "grad_norm": 18.37752342224121,
      "learning_rate": 2.86203941730934e-05,
      "loss": 1.8458,
      "step": 500
    },
    {
      "epoch": 1.3110539845758356,
      "grad_norm": 18.546775817871094,
      "learning_rate": 2.8191945158526133e-05,
      "loss": 1.9579,
      "step": 510
    },
    {
      "epoch": 1.3367609254498714,
      "grad_norm": 15.172959327697754,
      "learning_rate": 2.7763496143958872e-05,
      "loss": 1.7458,
      "step": 520
    },
    {
      "epoch": 1.3624678663239074,
      "grad_norm": 18.631288528442383,
      "learning_rate": 2.7335047129391605e-05,
      "loss": 1.9253,
      "step": 530
    },
    {
      "epoch": 1.3881748071979434,
      "grad_norm": 16.557445526123047,
      "learning_rate": 2.6906598114824337e-05,
      "loss": 1.7616,
      "step": 540
    },
    {
      "epoch": 1.4138817480719794,
      "grad_norm": 15.728169441223145,
      "learning_rate": 2.647814910025707e-05,
      "loss": 1.5735,
      "step": 550
    },
    {
      "epoch": 1.4395886889460154,
      "grad_norm": 15.421510696411133,
      "learning_rate": 2.6049700085689806e-05,
      "loss": 1.8045,
      "step": 560
    },
    {
      "epoch": 1.4652956298200515,
      "grad_norm": 17.79245948791504,
      "learning_rate": 2.562125107112254e-05,
      "loss": 1.8582,
      "step": 570
    },
    {
      "epoch": 1.4910025706940875,
      "grad_norm": 16.176660537719727,
      "learning_rate": 2.519280205655527e-05,
      "loss": 1.6273,
      "step": 580
    },
    {
      "epoch": 1.5167095115681235,
      "grad_norm": 14.716952323913574,
      "learning_rate": 2.4764353041988007e-05,
      "loss": 1.7087,
      "step": 590
    },
    {
      "epoch": 1.5424164524421595,
      "grad_norm": 16.95928955078125,
      "learning_rate": 2.433590402742074e-05,
      "loss": 1.8376,
      "step": 600
    },
    {
      "epoch": 1.5681233933161953,
      "grad_norm": 17.225587844848633,
      "learning_rate": 2.3907455012853472e-05,
      "loss": 1.7656,
      "step": 610
    },
    {
      "epoch": 1.5938303341902313,
      "grad_norm": 17.18803596496582,
      "learning_rate": 2.3479005998286205e-05,
      "loss": 1.7695,
      "step": 620
    },
    {
      "epoch": 1.6195372750642674,
      "grad_norm": 16.703819274902344,
      "learning_rate": 2.3050556983718937e-05,
      "loss": 1.9005,
      "step": 630
    },
    {
      "epoch": 1.6452442159383034,
      "grad_norm": 18.962556838989258,
      "learning_rate": 2.262210796915167e-05,
      "loss": 1.9323,
      "step": 640
    },
    {
      "epoch": 1.6709511568123392,
      "grad_norm": 16.83018684387207,
      "learning_rate": 2.2193658954584406e-05,
      "loss": 1.9592,
      "step": 650
    },
    {
      "epoch": 1.6966580976863752,
      "grad_norm": 15.180277824401855,
      "learning_rate": 2.176520994001714e-05,
      "loss": 1.9081,
      "step": 660
    },
    {
      "epoch": 1.7223650385604112,
      "grad_norm": 17.913846969604492,
      "learning_rate": 2.133676092544987e-05,
      "loss": 1.6309,
      "step": 670
    },
    {
      "epoch": 1.7480719794344473,
      "grad_norm": 16.16640281677246,
      "learning_rate": 2.0908311910882604e-05,
      "loss": 1.8266,
      "step": 680
    },
    {
      "epoch": 1.7737789203084833,
      "grad_norm": 17.653430938720703,
      "learning_rate": 2.047986289631534e-05,
      "loss": 1.8964,
      "step": 690
    },
    {
      "epoch": 1.7994858611825193,
      "grad_norm": 19.426898956298828,
      "learning_rate": 2.0051413881748076e-05,
      "loss": 1.7051,
      "step": 700
    },
    {
      "epoch": 1.8251928020565553,
      "grad_norm": 14.38988208770752,
      "learning_rate": 1.9622964867180808e-05,
      "loss": 1.81,
      "step": 710
    },
    {
      "epoch": 1.8508997429305913,
      "grad_norm": 16.796491622924805,
      "learning_rate": 1.919451585261354e-05,
      "loss": 1.8977,
      "step": 720
    },
    {
      "epoch": 1.8766066838046274,
      "grad_norm": 15.458815574645996,
      "learning_rate": 1.8766066838046273e-05,
      "loss": 1.7732,
      "step": 730
    },
    {
      "epoch": 1.9023136246786634,
      "grad_norm": 16.627607345581055,
      "learning_rate": 1.8337617823479006e-05,
      "loss": 1.8431,
      "step": 740
    },
    {
      "epoch": 1.9280205655526992,
      "grad_norm": 12.96683406829834,
      "learning_rate": 1.7909168808911742e-05,
      "loss": 1.5823,
      "step": 750
    },
    {
      "epoch": 1.9537275064267352,
      "grad_norm": 15.372673988342285,
      "learning_rate": 1.7480719794344475e-05,
      "loss": 1.8304,
      "step": 760
    },
    {
      "epoch": 1.9794344473007712,
      "grad_norm": 14.543523788452148,
      "learning_rate": 1.7052270779777207e-05,
      "loss": 1.5894,
      "step": 770
    },
    {
      "epoch": 2.005141388174807,
      "grad_norm": 17.075910568237305,
      "learning_rate": 1.662382176520994e-05,
      "loss": 1.79,
      "step": 780
    },
    {
      "epoch": 2.030848329048843,
      "grad_norm": 17.26706886291504,
      "learning_rate": 1.6195372750642672e-05,
      "loss": 1.4321,
      "step": 790
    },
    {
      "epoch": 2.056555269922879,
      "grad_norm": 15.42612075805664,
      "learning_rate": 1.5766923736075408e-05,
      "loss": 1.5816,
      "step": 800
    },
    {
      "epoch": 2.082262210796915,
      "grad_norm": 17.582603454589844,
      "learning_rate": 1.533847472150814e-05,
      "loss": 1.4883,
      "step": 810
    },
    {
      "epoch": 2.107969151670951,
      "grad_norm": 18.088115692138672,
      "learning_rate": 1.4910025706940875e-05,
      "loss": 1.5522,
      "step": 820
    },
    {
      "epoch": 2.133676092544987,
      "grad_norm": 13.864522933959961,
      "learning_rate": 1.4481576692373608e-05,
      "loss": 1.6272,
      "step": 830
    },
    {
      "epoch": 2.159383033419023,
      "grad_norm": 17.61518669128418,
      "learning_rate": 1.405312767780634e-05,
      "loss": 1.5004,
      "step": 840
    },
    {
      "epoch": 2.185089974293059,
      "grad_norm": 18.353757858276367,
      "learning_rate": 1.3624678663239075e-05,
      "loss": 1.3507,
      "step": 850
    },
    {
      "epoch": 2.210796915167095,
      "grad_norm": 15.721917152404785,
      "learning_rate": 1.3196229648671809e-05,
      "loss": 1.4334,
      "step": 860
    },
    {
      "epoch": 2.236503856041131,
      "grad_norm": 16.481245040893555,
      "learning_rate": 1.2767780634104543e-05,
      "loss": 1.2642,
      "step": 870
    },
    {
      "epoch": 2.2622107969151672,
      "grad_norm": 18.271156311035156,
      "learning_rate": 1.2339331619537276e-05,
      "loss": 1.5198,
      "step": 880
    },
    {
      "epoch": 2.2879177377892033,
      "grad_norm": 15.262493133544922,
      "learning_rate": 1.191088260497001e-05,
      "loss": 1.5121,
      "step": 890
    },
    {
      "epoch": 2.3136246786632393,
      "grad_norm": 16.02570152282715,
      "learning_rate": 1.1482433590402743e-05,
      "loss": 1.409,
      "step": 900
    },
    {
      "epoch": 2.3393316195372753,
      "grad_norm": 14.391722679138184,
      "learning_rate": 1.1053984575835475e-05,
      "loss": 1.4352,
      "step": 910
    },
    {
      "epoch": 2.365038560411311,
      "grad_norm": 14.843199729919434,
      "learning_rate": 1.062553556126821e-05,
      "loss": 1.4397,
      "step": 920
    },
    {
      "epoch": 2.390745501285347,
      "grad_norm": 13.743783950805664,
      "learning_rate": 1.0197086546700944e-05,
      "loss": 1.3711,
      "step": 930
    },
    {
      "epoch": 2.416452442159383,
      "grad_norm": 13.60107707977295,
      "learning_rate": 9.768637532133676e-06,
      "loss": 1.5562,
      "step": 940
    },
    {
      "epoch": 2.442159383033419,
      "grad_norm": 15.034290313720703,
      "learning_rate": 9.34018851756641e-06,
      "loss": 1.5755,
      "step": 950
    },
    {
      "epoch": 2.467866323907455,
      "grad_norm": 15.134203910827637,
      "learning_rate": 8.911739502999143e-06,
      "loss": 1.4076,
      "step": 960
    },
    {
      "epoch": 2.493573264781491,
      "grad_norm": 19.423118591308594,
      "learning_rate": 8.483290488431877e-06,
      "loss": 1.4829,
      "step": 970
    },
    {
      "epoch": 2.519280205655527,
      "grad_norm": 14.30817985534668,
      "learning_rate": 8.05484147386461e-06,
      "loss": 1.5158,
      "step": 980
    },
    {
      "epoch": 2.544987146529563,
      "grad_norm": 14.134718894958496,
      "learning_rate": 7.6263924592973435e-06,
      "loss": 1.3985,
      "step": 990
    },
    {
      "epoch": 2.570694087403599,
      "grad_norm": 16.624805450439453,
      "learning_rate": 7.197943444730078e-06,
      "loss": 1.5373,
      "step": 1000
    },
    {
      "epoch": 2.596401028277635,
      "grad_norm": 17.745729446411133,
      "learning_rate": 6.76949443016281e-06,
      "loss": 1.6006,
      "step": 1010
    },
    {
      "epoch": 2.622107969151671,
      "grad_norm": 15.985358238220215,
      "learning_rate": 6.3410454155955455e-06,
      "loss": 1.5236,
      "step": 1020
    },
    {
      "epoch": 2.6478149100257067,
      "grad_norm": 14.935343742370605,
      "learning_rate": 5.912596401028278e-06,
      "loss": 1.4469,
      "step": 1030
    },
    {
      "epoch": 2.6735218508997427,
      "grad_norm": 15.789040565490723,
      "learning_rate": 5.4841473864610115e-06,
      "loss": 1.5599,
      "step": 1040
    },
    {
      "epoch": 2.6992287917737787,
      "grad_norm": 14.488101959228516,
      "learning_rate": 5.055698371893745e-06,
      "loss": 1.4616,
      "step": 1050
    },
    {
      "epoch": 2.7249357326478147,
      "grad_norm": 13.662943840026855,
      "learning_rate": 4.627249357326478e-06,
      "loss": 1.4743,
      "step": 1060
    },
    {
      "epoch": 2.7506426735218508,
      "grad_norm": 19.745668411254883,
      "learning_rate": 4.198800342759212e-06,
      "loss": 1.49,
      "step": 1070
    },
    {
      "epoch": 2.776349614395887,
      "grad_norm": 17.0368595123291,
      "learning_rate": 3.7703513281919457e-06,
      "loss": 1.5458,
      "step": 1080
    },
    {
      "epoch": 2.802056555269923,
      "grad_norm": 17.98678970336914,
      "learning_rate": 3.3419023136246787e-06,
      "loss": 1.5825,
      "step": 1090
    },
    {
      "epoch": 2.827763496143959,
      "grad_norm": 14.943002700805664,
      "learning_rate": 2.913453299057412e-06,
      "loss": 1.5684,
      "step": 1100
    },
    {
      "epoch": 2.853470437017995,
      "grad_norm": 17.41019058227539,
      "learning_rate": 2.485004284490146e-06,
      "loss": 1.6511,
      "step": 1110
    },
    {
      "epoch": 2.879177377892031,
      "grad_norm": 18.687280654907227,
      "learning_rate": 2.056555269922879e-06,
      "loss": 1.6473,
      "step": 1120
    },
    {
      "epoch": 2.904884318766067,
      "grad_norm": 15.24474048614502,
      "learning_rate": 1.6281062553556129e-06,
      "loss": 1.5215,
      "step": 1130
    },
    {
      "epoch": 2.930591259640103,
      "grad_norm": 15.441754341125488,
      "learning_rate": 1.1996572407883463e-06,
      "loss": 1.4484,
      "step": 1140
    },
    {
      "epoch": 2.956298200514139,
      "grad_norm": 16.250272750854492,
      "learning_rate": 7.712082262210797e-07,
      "loss": 1.4505,
      "step": 1150
    },
    {
      "epoch": 2.982005141388175,
      "grad_norm": 17.26265525817871,
      "learning_rate": 3.427592116538132e-07,
      "loss": 1.4194,
      "step": 1160
    }
  ],
  "logging_steps": 10,
  "max_steps": 1167,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 50,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 29722989312000.0,
  "train_batch_size": 2,
  "trial_name": null,
  "trial_params": null
}
